{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4854e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e9b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Values taken as input are scaled using one hot encoding\n",
    "## Numerical Values Taken as input are converted using min max scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# select the features and target columns\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, return_dict=True)\n",
    "#print(f'Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "#print('Test loss: {:.4f}, Test MAE: {:.4f}'.format(test_loss, test_mae))\n",
    "print(test_loss)\n",
    "print(test_mae)\n",
    "\n",
    "\n",
    "# use the model to make predictions on new data\n",
    "#new_data = np.array([[...]]) # replace with actual data\n",
    "#predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# extract the MAE values from the history\n",
    "mae = history.history['mae']\n",
    "\n",
    "# plot the MAE values\n",
    "#plt.plot(mae)\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('MAE')\n",
    "#plt.show()\n",
    "\n",
    "# create a scatter plot with the epoch numbers on the x-axis and the MAE values on the y-axis\n",
    "plt.scatter(range(1, len(mae) + 1), mae)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce66a4b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the data and split it into training and testing sets\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Deaths\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart Date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd Date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDis Mag Value\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Deaths\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Injured\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Homeless\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Affected\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Affected\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRank\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#y = df3['Rank']\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRank\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "##Not working Rn  \n",
    "#Need to scale the target values for this to work \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the data and split it into training and testing sets\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df['Rank']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Should work with scaled values \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "#X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Total Deaths']\n",
    "\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[ 'scaled_Dis Mag Value']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c753b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_logarithmic_error'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985feb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab779d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experimental This is not working \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume you have a dataframe 'df' with columns 'feature1', 'feature2', ..., 'target'\n",
    "# and a list of the column names of the features called 'feature_cols'\n",
    "\n",
    "# Split the data into features and target\n",
    "X = dfle[['ISO', 'Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "y = dfle['Timezone']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use the 'predict' method to make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# You can also use the 'predict_proba' method to predict the class probabilities\n",
    "probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data using the 'accuracy_score' function from the 'sklearn.metrics' module\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cbaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding using one hot encoding\n",
    "df3 = pd.get_dummies(df2, columns=['Disaster Subgroup', 'Disaster Type', 'Country', 'ISO', 'Region', 'Continent','Dis Mag Scale','Timezone'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling using Min Max \n",
    "# Assuming your data is stored in a Pandas dataframe called \"df\"\n",
    "\n",
    "# Select the features that you want to scale\n",
    "features = df3[['Dis Mag Value', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "       'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'CPI', 'Rank']]\n",
    "\n",
    "# Iterate through each feature\n",
    "for feature in features:\n",
    "  # Select the feature\n",
    "  col = df3[feature]\n",
    "\n",
    "  # Find the minimum and maximum values for the column\n",
    "  min_value = col.min()\n",
    "  max_value = col.max()\n",
    "\n",
    "  # Subtract the minimum value from the column\n",
    "  scaled_col = col - min_value\n",
    "\n",
    "  # Divide by the range (max - min)\n",
    "  scaled_col = scaled_col / (max_value - min_value)\n",
    "\n",
    "  # Assign the scaled values to a new column in the dataframe\n",
    "  df3[f'scaled_{feature}'] = scaled_col\n",
    "df3.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91059dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf746fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e00058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
