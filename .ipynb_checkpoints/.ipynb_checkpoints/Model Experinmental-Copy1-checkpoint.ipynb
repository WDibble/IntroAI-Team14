{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248fc78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca77c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Values taken as input are scaled using one hot encoding\n",
    "## Numerical Values Taken as input are converted using min max scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# select the features and target columns\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, return_dict=True)\n",
    "#print(f'Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "#print('Test loss: {:.4f}, Test MAE: {:.4f}'.format(test_loss, test_mae))\n",
    "print(test_loss)\n",
    "print(test_mae)\n",
    "\n",
    "\n",
    "# use the model to make predictions on new data\n",
    "#new_data = np.array([[...]]) # replace with actual data\n",
    "#predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ad798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# extract the MAE values from the history\n",
    "mae = history.history['mae']\n",
    "\n",
    "# plot the MAE values\n",
    "#plt.plot(mae)\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('MAE')\n",
    "#plt.show()\n",
    "\n",
    "# create a scatter plot with the epoch numbers on the x-axis and the MAE values on the y-axis\n",
    "plt.scatter(range(1, len(mae) + 1), mae)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d904b37a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the data and split it into training and testing sets\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Deaths\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart Date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd Date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDis Mag Value\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Deaths\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Injured\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Homeless\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Affected\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Affected\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRank\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#y = df3['Rank']\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRank\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "##Not working Rn  \n",
    "#Need to scale the target values for this to work \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the data and split it into training and testing sets\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df['Rank']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb63fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Should work with scaled values \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "#X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Total Deaths']\n",
    "\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[ 'scaled_Dis Mag Value']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_logarithmic_error'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2545358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experimental This is not working \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume you have a dataframe 'df' with columns 'feature1', 'feature2', ..., 'target'\n",
    "# and a list of the column names of the features called 'feature_cols'\n",
    "\n",
    "# Split the data into features and target\n",
    "X = dfle[['ISO', 'Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "y = dfle['Timezone']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use the 'predict' method to make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# You can also use the 'predict_proba' method to predict the class probabilities\n",
    "probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data using the 'accuracy_score' function from the 'sklearn.metrics' module\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa23327",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding using one hot encoding\n",
    "df = pd.get_dummies(df, columns=['Disaster Subgroup', 'Disaster Type', 'Country', 'ISO', 'Region', 'Continent','Dis Mag Scale','Timezone'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c92087",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling using Min Max \n",
    "# Assuming your data is stored in a Pandas dataframe called \"df\"\n",
    "\n",
    "# Select the features that you want to scale\n",
    "features = df[['Dis Mag Value', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "       'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'CPI', 'Rank']]\n",
    "\n",
    "# Iterate through each feature\n",
    "for feature in features:\n",
    "  # Select the feature\n",
    "  col = df[feature]\n",
    "\n",
    "  # Find the minimum and maximum values for the column\n",
    "  min_value = col.min()\n",
    "  max_value = col.max()\n",
    "\n",
    "  # Subtract the minimum value from the column\n",
    "  scaled_col = col - min_value\n",
    "\n",
    "  # Divide by the range (max - min)\n",
    "  scaled_col = scaled_col / (max_value - min_value)\n",
    "\n",
    "  # Assign the scaled values to a new column in the dataframe\n",
    "  df[f'scaled_{feature}'] = scaled_col\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0544f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# select the features and target columns\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, return_dict=True)\n",
    "#print(f'Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "#print('Test loss: {:.4f}, Test MAE: {:.4f}'.format(test_loss, test_mae))\n",
    "print(test_loss)\n",
    "print(test_mae)\n",
    "\n",
    "\n",
    "# use the model to make predictions on new data\n",
    "#new_data = np.array([[...]]) # replace with actual data\n",
    "#predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab003aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need to preprocess the data first\n",
    "\n",
    "# create a Pandas data frame with the input data for the new samples\n",
    "lol = df = pd.read_csv('2022_DISASTERS_TARGET.csv')\n",
    "\n",
    "\n",
    "# convert the data frame to a NumPy array\n",
    "new_data = lol.values\n",
    "\n",
    "# use the model to make predictions on the new data\n",
    "predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# extract the MAE values from the history\n",
    "mae = history.history['mae']\n",
    "\n",
    "# plot the MAE values\n",
    "plt.plot(mae)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()\n",
    "\n",
    "# create a scatter plot with the epoch numbers on the x-axis and the MAE values on the y-axis\n",
    "#plt.scatter(range(1, len(mae) + 1), mae)\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('MAE')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "#X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Total Deaths']\n",
    "\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[ 'scaled_Dis Mag Value']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c5ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df3[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_logarithmic_error'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26440d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Select the categorical columns\n",
    "cat_cols = df2[['Disaster Subgroup', 'Disaster Type', 'Country', 'ISO', 'Region', 'Continent','Dis Mag Scale']]\n",
    "\n",
    "# Create the OneHotEncoder object\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "onehot_encoded = onehot_encoder.fit_transform(df2[cat_cols])\n",
    "\n",
    "# Add the one-hot encoded columns to the original dataframe\n",
    "df2_onehot = pd.concat([df2, pd.DataFrame(onehot_encoded)], axis=1)\n",
    "\n",
    "# Drop the categorical columns\n",
    "df2_onehot = df2_onehot.drop(cat_cols, axis=1)\n",
    "\n",
    "\n",
    "# Display the resulting dataframe\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select the numerical columns\n",
    "num_cols = ['Aid Contribution', 'Dis Mag Value', 'Dis Mag Scale', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "            'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'Insured Damages ('000 US$)', \n",
    "            'Total Damages ('000 US$)', 'CPI']\n",
    "\n",
    "# Create the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the numerical columns\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087335d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing categorical values to numerical values\n",
    "dfle = df\n",
    "dfle.Continent = le.fit_transform(dfle.Continent)\n",
    "dfle.ISO= le.fit_transform(dfle.ISO)\n",
    "dfle.Region= le.fit_transform(dfle.Region)\n",
    "dfle['Disaster Type']= le.fit_transform(dfle['Disaster Subgroup'])\n",
    "dfle['Disaster Subgroup']= le.fit_transform(dfle['Disaster Subgroup'])\n",
    "dfle['Start Date']= le.fit_transform(dfle['Start Date'])\n",
    "dfle['End Date']= le.fit_transform(dfle['End Date'])\n",
    "dfle['Timezone']= le.fit_transform(dfle['Timezone'])\n",
    "dfle['Country']= le.fit_transform(dfle['Country'])\n",
    "dfle['Dis Mag Scale']= le.fit_transform(dfle['Dis Mag Scale'])\n",
    "dfle.head()\n",
    "dfle.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to a datetime type\n",
    "df3['Start Date'] = pd.to_datetime(df3['Start Date'])\n",
    "\n",
    "# Extract the month from each date\n",
    "df3['Year'] = df3['Start Date'].dt.year\n",
    "\n",
    "F = df3[['Year','Longitude','Latitude']]\n",
    "#F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to a datetime type\n",
    "df3['Start Date'] = pd.to_datetime(df3['Start Date'])\n",
    "\n",
    "# Extract the month from each date\n",
    "df3['Year'] = df3['Start Date'].dt.year\n",
    "\n",
    "F = df3[['Year','Longitude','Latitude']]\n",
    "#F\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset and extract the input features and target variable\n",
    "#X = dfle[['ISO','Total Deaths','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "#X = df3.drop(['Total Deaths'], axis=1)  \n",
    "#y = df3['Total Deaths']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume you have a dataframe 'df' with columns 'feature1', 'feature2', ..., 'target'\n",
    "# and a list of the column names of the features called 'feature_cols'\n",
    "\n",
    "# Split the data into features and target\n",
    "X = dfle[['ISO', 'Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "y = dfle['Timezone']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use the 'predict' method to make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# You can also use the 'predict_proba' method to predict the class probabilities\n",
    "probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data using the 'accuracy_score' function from the 'sklearn.metrics' module\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
